# ============================================================================
# NEUROCLIMA DOCUMENT PROCESSOR - ENVIRONMENT CONFIGURATION
# ============================================================================
# Copy this file to .env and fill in your actual values
# Never commit the .env file to version control
# ============================================================================

# ----------------------------------------------------------------------------
# Application Settings
# ----------------------------------------------------------------------------
APP_NAME=NeuroClima Document Processor
APP_VERSION=7.0.0
DEBUG=False
LOG_LEVEL=INFO
HOST=0.0.0.0
PORT=5000

# ----------------------------------------------------------------------------
# Processing Configuration
# ----------------------------------------------------------------------------
MAX_CONCURRENT_TASKS=1

# ----------------------------------------------------------------------------
# MODEL PROVIDER SELECTION (Free vs Paid)
# ----------------------------------------------------------------------------
# Options: "free" (Ollama) or "paid" (OpenAI)
MODEL_PROVIDER=free

# When MODEL_PROVIDER=free, uses Ollama (local, no cost)
# When MODEL_PROVIDER=paid, uses OpenAI API (requires API key, incurs costs)

# ----------------------------------------------------------------------------
# OpenAI Configuration (for MODEL_PROVIDER=paid)
# ----------------------------------------------------------------------------
# OpenAI API Key (required when MODEL_PROVIDER=paid)
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI model for text generation and summarization
OPENAI_MODEL=gpt-4o

# OpenAI embedding model
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# OpenAI API timeout in MINUTES (will be converted to seconds)
OPENAI_TIMEOUT=2

# OpenAI API base URL (optional, for custom endpoints)
OPENAI_API_BASE=https://api.openai.com/v1/chat/completions

# ----------------------------------------------------------------------------
# Ollama (LLM & Embeddings) - for MODEL_PROVIDER=free
# ----------------------------------------------------------------------------
# Ollama server URL (external GPU server for fast retrieval)
OLLAMA_API_URL=your_ollama_server_url_here

# LLM model for text generation and summarization
OLLAMA_MODEL=mistral:7b

# Embedding model for search/retrieval queries (NOT document processing)
# Using Qwen3-Embedding-0.6B (1024 dimensions)
# Used by: /search/chunks, /search/summaries, /search/hybrid endpoints
# Document processing uses local models (see Local Embedding Models section below)
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b
OLLAMA_EMBEDDING_DIM=1024
OLLAMA_EMBEDDING_BATCH_SIZE=32

# API timeout in MINUTES (will be converted to seconds)
OLLAMA_TIMEOUT=20000

OLLAMA_HOST=your_ollama_server_host_here
OLLAMA_PORT=11434

# ----------------------------------------------------------------------------
# Local Embedding Models (Loaded at Startup)
# ----------------------------------------------------------------------------
# Choose embedding mode for document processing:
#   - "local": Load models locally at startup (requires model downloads, transformers>=4.44.0)
#   - "external": Use external Ollama API for everything (simpler, no dependencies)
# Search/retrieval queries ALWAYS use external Ollama regardless of this setting
PROCESSOR_EMBEDDING_MODE=external

# IMPORTANT: These models are loaded at startup for DOCUMENT PROCESSING ONLY when mode=local
# Search/retrieval queries use external Ollama API (configured above)
#
# Architecture:
#   - Document Processing (batch): Local HuggingFace models (if mode=local) OR external Ollama (if mode=external)
#   - Search/Retrieval (queries): External Ollama API (OLLAMA_API_URL above)

# Main document embedding model (for processing chunks and summaries)
# Only used when PROCESSOR_EMBEDDING_MODE=local
# Using HuggingFace Qwen3-Embedding-0.6B model loaded locally
LOCAL_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
LOCAL_EMBEDDING_DIM=1024
LOCAL_EMBEDDING_BATCH_SIZE=32
LOCAL_EMBEDDING_MAX_SEQ_LENGTH=512
LOCAL_EMBEDDING_DEVICE=  # Leave empty for auto-detect (cuda/cpu)

# STP embedding model (loaded locally for STP document processing/storage)
# Only used when PROCESSOR_EMBEDDING_MODE=local
# Using sentence-transformers all-MiniLM-L6-v2 model
# Note: STP search/retrieval uses external Ollama API (see STP_EMBEDDING_API_BASE below)
STP_LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
STP_EMBEDDING_BATCH_SIZE=32
STP_EMBEDDING_MAX_SEQ_LENGTH=256
STP_EMBEDDING_DEVICE=  # Leave empty for auto-detect (cuda/cpu)

# ----------------------------------------------------------------------------
# MinIO (Object Storage)
# ----------------------------------------------------------------------------
# MinIO server endpoint (without http://)
MINIO_ENDPOINT=your_minio_server_endpoint_here

# MinIO access credentials
ACCESS_KEY=your_minio_access_key_here
SECRET_KEY=your_minio_secret_key_here

# Use HTTPS for MinIO connection
SECURE=True

# Processable buckets (comma-separated)
PROCESSABLE_BUCKETS=researchpapers,policy,news,scientificdata

# ----------------------------------------------------------------------------
# Milvus (Vector Database)
# ----------------------------------------------------------------------------
# Milvus server connection
MILVUS_HOST=your_milvus_server_host_here
MILVUS_PORT=19530

# Milvus authentication (leave empty if not required)
MILVUS_USER=root
MILVUS_PASSWORD=Milvus

# Milvus database names
MILVUS_CHUNK_DATABASE=chunk_prod
MILVUS_SUMMARY_DATABASE=summary_prod

# Milvus collection mapping
MILVUS_COLLECTION_NEWS=News
MILVUS_COLLECTION_SCIENTIFICDATA=Scientific_Data
MILVUS_COLLECTION_POLICY=Policy
MILVUS_COLLECTION_RESEARCHPAPERS=Research_Papers

# ----------------------------------------------------------------------------
# Unstructured API (Document Extraction)
# ----------------------------------------------------------------------------
# Unstructured API server URL
UNSTRUCTURED_API_URL=http://localhost:9000

# API timeout in MINUTES
UNSTRUCTURED_TIMEOUT=5000

# ----------------------------------------------------------------------------
# VISION MODEL & IMAGE EXTRACTION
# ----------------------------------------------------------------------------
# Enable image extraction and description
ENABLE_IMAGE_EXTRACTION=True

# Vision model provider: "ollama" (llava, bakllava) or "openai" (gpt-4-vision)
VISION_MODEL_PROVIDER=ollama

# Ollama Vision Model (when VISION_MODEL_PROVIDER=ollama)
# Options: llava:13b, llava:7b, bakllava, llava-llama3
OLLAMA_VISION_MODEL=gemma3:4b

# OpenAI Vision Model (when VISION_MODEL_PROVIDER=openai)
OPENAI_VISION_MODEL=gpt-4-vision-preview

# Image extraction settings
EXTRACT_IMAGES_FROM_PDF=True
EXTRACT_IMAGES_FROM_DOCX=True
IMAGE_MIN_WIDTH=100
IMAGE_MIN_HEIGHT=100
IMAGE_MAX_SIZE_MB=10

# Image processing
RESIZE_IMAGES_FOR_VISION=True
MAX_IMAGE_DIMENSION=1024

# Replace images with descriptions in final output
REPLACE_IMAGES_WITH_DESCRIPTIONS=True

# ----------------------------------------------------------------------------
# CLIMATEGPT-7B MODEL (Advanced Summarization)
# ----------------------------------------------------------------------------
# Enable ClimateGPT-7B for high-quality abstractive summaries
# Model: https://huggingface.co/eci-io/climategpt-7b
USE_CLIMATEGPT=True

# Device to run ClimateGPT on ('auto', 'cuda', or 'cpu')
# 'auto' will automatically use GPU if available, otherwise CPU
CLIMATEGPT_DEVICE=auto

# Memory optimization (use one of these for large models)
CLIMATEGPT_8BIT=False
CLIMATEGPT_4BIT=False

# ----------------------------------------------------------------------------
# SCIENTIFIC DATA PROCESSING
# ----------------------------------------------------------------------------
# Enable specialized processing for scientific data files
ENABLE_SCIENTIFIC_DATA_PROCESSING=True

# Supported scientific data formats
SCIENTIFIC_DATA_FORMATS=csv,tsv,xlsx,xls,nc,netcdf,hdf5,h5,mat,parquet

# CSV/Excel processing
CSV_SAMPLE_ROWS=100
CSV_INCLUDE_STATISTICS=True
EXCEL_PROCESS_ALL_SHEETS=True

# NetCDF/HDF5 processing
NETCDF_EXTRACT_METADATA=True
NETCDF_SAMPLE_ARRAYS=True
NETCDF_MAX_ARRAY_SAMPLE=1000

# Scientific data chunking strategy
SCIENTIFIC_DATA_CHUNK_SIZE=5000
SCIENTIFIC_DATA_INCLUDE_SCHEMA=True
SCIENTIFIC_DATA_INCLUDE_STATISTICS=True

# Python libraries required:
# pandas, xarray, netCDF4, h5py, openpyxl, pyarrow

# ----------------------------------------------------------------------------
# GraphRAG Settings
# ----------------------------------------------------------------------------
# Enable Microsoft GraphRAG processing
ENABLE_MICROSOFT_GRAPHRAG=True

# GraphRAG workspace directories

GRAPHRAG_BASE_DIR=./graphrag_workspace
GRAPHRAG_TEMP_DIR=./graphrag_temp

# ============================================================================
# GraphRAG QUERY Models (for search/retrieval operations)
# ============================================================================

# Chat model for answering queries (local-search, global-search, etc.)
# IMPORTANT: Must include /v1 suffix for OpenAI-compatible API
GRAPHRAG_QUERY_CHAT_MODEL_API_BASE=http://localhost:11434/v1
GRAPHRAG_QUERY_CHAT_MODEL_API_KEY=ollama
GRAPHRAG_QUERY_CHAT_MODEL=qwen2.5:0.5b


# Embedding model for query vector similarity search
# IMPORTANT: Must include /v1 suffix for OpenAI-compatible API
GRAPHRAG_QUERY_EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1
GRAPHRAG_QUERY_EMBEDDING_MODEL_API_KEY=ollama
GRAPHRAG_QUERY_EMBEDDING_MODEL=nomic-embed-text:latest

# ============================================================================
# GraphRAG INDEXING Models (for document processing/entity extraction)
# ============================================================================

# Chat model for entity/relationship extraction during indexing
# IMPORTANT: Must include /v1 suffix for OpenAI-compatible API
GRAPHRAG_INDEXING_CHAT_MODEL_API_BASE=http://localhost:11434/v1
GRAPHRAG_INDEXING_CHAT_MODEL_API_KEY=ollama
GRAPHRAG_INDEXING_CHAT_MODEL=mistral:7b

# Embedding model for creating vector embeddings during indexing
# IMPORTANT: Must include /v1 suffix for OpenAI-compatible API
GRAPHRAG_EMBEDDING_MODEL_API_BASE=http://localhost:11434/v1
GRAPHRAG_EMBEDDING_MODEL_API_KEY=ollama
GRAPHRAG_EMBEDDING_MODEL=nomic-embed-text:latest

# GraphRAG Processing Configuration

GRAPHRAG_LLM_TIMEOUT=3000000
GRAPHRAG_LLM_TEMPERATURE=0.1
GRAPHRAG_LLM_MAX_TOKENS=4000
GRAPHRAG_MAX_TEXT_LENGTH=50000
GRAPHRAG_CHUNK_SIZE=1200
GRAPHRAG_TIMEOUT=6000000

# GraphRAG Advanced Features

GRAPHRAG_ENABLE_CLAIMS=True
GRAPHRAG_ENABLE_COVARIATES=True
GRAPHRAG_ENABLE_TEXT_UNITS=True
GRAPHRAG_ENABLE_ENTITY_EMBEDDINGS=True

# ----------------------------------------------------------------------------
# STP (Social Tipping Points) Settings
# ----------------------------------------------------------------------------
# Enable STP processing
ENABLE_STP=True

# STP Milvus Configuration
STP_MILVUS_DATABASE=stp_prod
STP_MILVUS_COLLECTION=stp_documents

# STP Classifier Configuration
STP_CLASSIFIER_MODEL=models/onnx_exports/roBERTa_stp0.5.onnx
STP_MIN_CONFIDENCE=0.5

# STP Text Processing
STP_TEXT_CLEANING_ENABLED=True
STP_REPHRASING_ENABLED=True
STP_REPHRASE_MAX_WORDS=80
STP_QF_ENABLED=True

# STP Embedding Configuration
# For document processing: Uses local sentence-transformers model (configured above in Local Embedding Models)
# For search/retrieval: Uses external Ollama API (configured below)
STP_EMBEDDING_MODEL=all-minilm:l6-v2
STP_EMBEDDING_DIM=384
STP_EMBEDDING_API_BASE=http://86.50.23.167:11434

# STP Chunking Configuration
STP_MIN_CHUNK_TOKENS=200
STP_MAX_CHUNK_TOKENS=1500
STP_TARGET_CHUNK_TOKENS=800

# STP Timeout in MINUTES (will be converted to seconds)
STP_TIMEOUT=5

# ----------------------------------------------------------------------------
# Chunking Strategy Configuration (per bucket)
# ----------------------------------------------------------------------------
# News
CHUNKING_STRATEGY_NEWS=semantic
CHUNK_SIZE_NEWS=600
CHUNK_OVERLAP_NEWS=100

# Scientific Data
CHUNKING_STRATEGY_SCIENTIFICDATA=semantic
CHUNK_SIZE_SCIENTIFICDATA=800
CHUNK_OVERLAP_SCIENTIFICDATA=150

# Policy
CHUNKING_STRATEGY_POLICY=semantic
CHUNK_SIZE_POLICY=700
CHUNK_OVERLAP_POLICY=120

# Research Papers
CHUNKING_STRATEGY_RESEARCHPAPERS=semantic
CHUNK_SIZE_RESEARCHPAPERS=900
CHUNK_OVERLAP_RESEARCHPAPERS=150

# ----------------------------------------------------------------------------
# Summarization Configuration (per bucket)
# ----------------------------------------------------------------------------
# Abstractive summarization (LLM-based)
SUMMARIZATION_NEWS=abstractive
SUMMARIZATION_SCIENTIFICDATA=abstractive
SUMMARIZATION_POLICY=abstractive
SUMMARIZATION_RESEARCHPAPERS=abstractive

# ----------------------------------------------------------------------------
# MongoDB Configuration (Persistent Data Storage)
# ----------------------------------------------------------------------------
# MongoDB connection for multi-replica support
# Stores: document_status, news_articles_status (replaces SQLite processing_tracker.db)

# For local development with MongoDB Compass (no auth)
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=neuroclimabot

# For authenticated connections (production/Kubernetes)
# Leave empty for no authentication (local dev)
MONGODB_USERNAME=
MONGODB_PASSWORD=

# Connection pool settings (optional, defaults shown)
MONGODB_MAX_POOL_SIZE=100
MONGODB_MIN_POOL_SIZE=10
MONGODB_SERVER_SELECTION_TIMEOUT=5000
MONGODB_CONNECT_TIMEOUT=10000

# ----------------------------------------------------------------------------
# Security & Performance
# ----------------------------------------------------------------------------
# CORS allowed origins (comma-separated, use * for all)
CORS_ORIGINS=*

# API rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=600

# Maximum file upload size (MB)
MAX_UPLOAD_SIZE_MB=100

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# Log file path (leave empty for console only)
LOG_FILE=

# Log format: json or text
LOG_FORMAT=text

# Log rotation (size in MB)
LOG_MAX_SIZE_MB=50
LOG_BACKUP_COUNT=3