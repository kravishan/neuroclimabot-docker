# TruLens RAG Evaluation for NeuroClimaBot

## ğŸ“š Table of Contents

1. [Overview](#overview)
2. [How TruLens Works](#how-trulens-works)
3. [Quick Start](#quick-start)
4. [Architecture](#architecture)
5. [Usage Examples](#usage-examples)
6. [Understanding Scores](#understanding-scores)
7. [Dashboard](#dashboard)
8. [API Reference](#api-reference)

---

## Overview

This module provides **TruLens** integration for evaluating your RAG (Retrieval-Augmented Generation) system in real-time. TruLens uses the **RAG Triad** methodology to comprehensively evaluate retrieval quality, answer groundedness, and relevance.

### What Gets Evaluated

```
Your RAG Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query                                          â”‚
â”‚     â†“                                                â”‚
â”‚  1. RETRIEVAL (Milvus + GraphRAG)   â† Evaluated     â”‚
â”‚     â†“                                                â”‚
â”‚  2. RERANKING                                        â”‚
â”‚     â†“                                                â”‚
â”‚  3. GENERATION (OpenAI/Mixtral)     â† Evaluated     â”‚
â”‚     â†“                                                â”‚
â”‚  Generated Answer                   â† Evaluated     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

âœ… **Real-time evaluation** of every RAG response
âœ… **Hallucination detection** via groundedness scoring
âœ… **Multi-source analysis** (Milvus vs GraphRAG quality comparison)
âœ… **Production-ready** with minimal performance overhead
âœ… **Dashboard visualization** for monitoring trends
âœ… **Seamless integration** with your existing RAG pipeline

---

## How TruLens Works

### The RAG Triad

TruLens evaluates your RAG system using three fundamental metrics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG TRIAD                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  1ï¸âƒ£  CONTEXT RELEVANCE (Retrieval Quality)         â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚      â”‚ Input:  User Query                  â”‚       â”‚
â”‚      â”‚ Output: Retrieved Contexts          â”‚       â”‚
â”‚      â”‚ Score:  0.0 - 1.0                   â”‚       â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚      Question: "Did we retrieve relevant docs?"    â”‚
â”‚                                                      â”‚
â”‚      Your System:                                   â”‚
â”‚      â€¢ Evaluates Milvus chunks separately          â”‚
â”‚      â€¢ Evaluates GraphRAG data separately          â”‚
â”‚      â€¢ Identifies which source performs better     â”‚
â”‚                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  2ï¸âƒ£  GROUNDEDNESS (Hallucination Detection)        â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚      â”‚ Input:  Retrieved Contexts          â”‚       â”‚
â”‚      â”‚ Output: Generated Answer            â”‚       â”‚
â”‚      â”‚ Score:  0.0 - 1.0                   â”‚       â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚      Question: "Is answer supported by context?"   â”‚
â”‚                                                      â”‚
â”‚      Detection:                                     â”‚
â”‚      â€¢ Score < 0.7 = Potential hallucination       â”‚
â”‚      â€¢ Automatic warnings logged                   â”‚
â”‚      â€¢ Tracked in statistics                       â”‚
â”‚                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  3ï¸âƒ£  ANSWER RELEVANCE (End-to-End Quality)        â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚      â”‚ Input:  User Query                  â”‚       â”‚
â”‚      â”‚ Output: Generated Answer            â”‚       â”‚
â”‚      â”‚ Score:  0.0 - 1.0                   â”‚       â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚      Question: "Does answer address the query?"    â”‚
â”‚                                                      â”‚
â”‚      Checks:                                        â”‚
â”‚      â€¢ Answer addresses the question               â”‚
â”‚      â€¢ Response is on-topic                        â”‚
â”‚      â€¢ User's intent is satisfied                  â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OVERALL SCORE = Average of all three metrics
```

### How It Works in Your Application

**Before TruLens:**
```python
User: "What are social tipping points?"
  â†“
RAG System processes query
  â†“
Response: "Social tipping points are..."
  â†“
â“ But was the answer accurate? Any hallucinations?
```

**With TruLens:**
```python
User: "What are social tipping points?"
  â†“
RAG System processes query
  â†“
TruLens Evaluator analyzes:
  â”œâ”€ Context Relevance: 0.85 âœ… (good retrieval)
  â”œâ”€ Groundedness: 0.92 âœ… (no hallucinations)
  â””â”€ Answer Relevance: 0.88 âœ… (addresses question)
  â†“
Response: "Social tipping points are..." + Quality Scores
  â†“
âœ… High confidence in answer quality!
```

---

## Quick Start

### 1. Enable TruLens in `.env`

**IMPORTANT:** TruLens is disabled by default. Enable it in `Server/.env`:

```bash
# Enable TruLens evaluation
TRULENS_ENABLED=true

# Optional: Configure settings
TRULENS_DB_PATH=./data/trulens_evaluations.db
TRULENS_EVALUATION_MODEL=gpt-4
TRULENS_GROUNDEDNESS_THRESHOLD=0.7
```

**To disable:** Set `TRULENS_ENABLED=false` (default)

See [QUICKSTART.md](./QUICKSTART.md) for detailed enable/disable instructions.

### 2. Installation

```bash
cd Server
pip install -r requirements.txt
```

This installs `trulens-eval==0.33.0` along with other dependencies.

### 3. Run Test

```bash
cd Server
python tests/test_trulens_integration.py
```

Expected output:
```
ğŸ“Š TruLens Evaluation Scores:
   â€¢ Context Relevance:  0.850 âœ…
   â€¢ Groundedness:       0.920 âœ…
   â€¢ Answer Relevance:   0.880 âœ…
   â€¢ Overall Score:      0.883 âœ…
```

### 3. Launch Dashboard

```bash
cd Server
python scripts/launch_trulens_dashboard.py
```

Open browser: `http://localhost:8501`

### 4. Integrate with Your RAG Service

See [INTEGRATION_GUIDE.md](./INTEGRATION_GUIDE.md) for detailed integration steps.

**Simple Integration:**

```python
# In Server/app/services/rag/chain.py

from app.services.evaluation.rag_evaluator import get_rag_evaluator

class CleanRAGService:
    async def initialize(self):
        # ... existing initialization ...
        self.evaluator = await get_rag_evaluator(enabled=True)

    async def query(self, question: str, **kwargs):
        # ... existing RAG processing ...
        result = await self.orchestrator.process_start_conversation(...)

        # Evaluate response
        if self.evaluator:
            scores = await self.evaluator.evaluate_from_rag_response(
                query=question,
                rag_response=result
            )
            if scores:
                result = self.evaluator.add_scores_to_response(result, scores)

        return result
```

---

## Architecture

### Module Structure

```
app/services/evaluation/
â”œâ”€â”€ __init__.py                    # Package exports
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ INTEGRATION_GUIDE.md           # Detailed integration guide
â”‚
â”œâ”€â”€ trulens_service.py             # Core TruLens service
â”‚   â”œâ”€â”€ TruLensService             # Main evaluation service
â”‚   â”œâ”€â”€ EvaluationScores           # Score dataclass
â”‚   â””â”€â”€ get_trulens_service()      # Singleton getter
â”‚
â”œâ”€â”€ trulens_custom_provider.py    # Custom Ollama/Mixtral provider
â”‚   â””â”€â”€ OllamaFeedbackProvider     # For users without OpenAI
â”‚
â””â”€â”€ rag_evaluator.py               # RAG-specific evaluator
    â”œâ”€â”€ RAGEvaluator               # High-level evaluator
    â””â”€â”€ get_rag_evaluator()        # Singleton getter
```

### Integration Points

```
Your Application:
â”œâ”€â”€ app/services/rag/chain.py
â”‚   â””â”€â”€ CleanRAGService            â† Add evaluator here
â”‚
â”œâ”€â”€ app/services/rag/orchestrator.py
â”‚   â””â”€â”€ RAGOrchestrator            â† Or add evaluator here
â”‚
â””â”€â”€ app/api/routes/query.py
    â””â”€â”€ query_endpoint()           â† Or add evaluator here
```

---

## Usage Examples

### Example 1: Basic Evaluation

```python
from app.services.rag.chain import get_rag_service
from app.services.evaluation.rag_evaluator import get_rag_evaluator

# Initialize
rag_service = await get_rag_service()
evaluator = await get_rag_evaluator(enabled=True)

# Query
result = await rag_service.query(
    question="What are the main causes of climate change?",
    language="en"
)

# Check scores
if "evaluation" in result:
    print(f"Context Relevance: {result['evaluation']['context_relevance']}")
    print(f"Groundedness: {result['evaluation']['groundedness']}")
    print(f"Answer Relevance: {result['evaluation']['answer_relevance']}")

    # Check for issues
    if result['quality_flags']['potential_hallucination']:
        print("âš ï¸ Warning: Potential hallucination detected!")
```

### Example 2: Compare Data Sources

```python
# After evaluation
eval_data = result['evaluation']

milvus_score = eval_data['milvus_context_relevance']
graphrag_score = eval_data['graphrag_context_relevance']

print(f"Milvus Quality: {milvus_score:.2f}")
print(f"GraphRAG Quality: {graphrag_score:.2f}")

if milvus_score > graphrag_score + 0.1:
    print("â†’ Milvus is providing better context")
elif graphrag_score > milvus_score + 0.1:
    print("â†’ GraphRAG is providing better context")
else:
    print("â†’ Both sources are performing similarly")
```

### Example 3: Quality Monitoring

```python
from app.services.evaluation.rag_evaluator import get_rag_evaluator

evaluator = await get_rag_evaluator()

# Get statistics
stats = evaluator.get_statistics()

print(f"Total Evaluations: {stats['total_evaluations']}")
print(f"Avg Groundedness: {stats['avg_groundedness']:.3f}")
print(f"Hallucination Rate: {stats['hallucination_rate']:.2%}")

# Alert if quality drops
if stats['avg_groundedness'] < 0.7:
    send_alert("RAG quality degraded - hallucination rate increased!")
```

### Example 4: A/B Testing

```python
# Test different prompts
results_prompt_a = []
results_prompt_b = []

for query in test_queries:
    # Test with Prompt A
    result_a = await rag_service.query(query, prompt_version="A")
    results_prompt_a.append(result_a['evaluation']['overall_score'])

    # Test with Prompt B
    result_b = await rag_service.query(query, prompt_version="B")
    results_prompt_b.append(result_b['evaluation']['overall_score'])

# Compare
avg_a = sum(results_prompt_a) / len(results_prompt_a)
avg_b = sum(results_prompt_b) / len(results_prompt_b)

print(f"Prompt A avg score: {avg_a:.3f}")
print(f"Prompt B avg score: {avg_b:.3f}")
print(f"Winner: {'Prompt B' if avg_b > avg_a else 'Prompt A'}")
```

---

## Understanding Scores

### Context Relevance Score

**Range:** 0.0 - 1.0

| Score | Meaning | Action |
|-------|---------|--------|
| 0.8 - 1.0 | Excellent retrieval | âœ… No action needed |
| 0.6 - 0.8 | Good retrieval | Consider tuning if trending down |
| 0.4 - 0.6 | Moderate retrieval | Review `SIMILARITY_THRESHOLD` |
| 0.0 - 0.4 | Poor retrieval | âš ï¸ Investigate retrieval config |

**Common Issues & Fixes:**

- **Low scores (< 0.6):**
  - Lower `SIMILARITY_THRESHOLD` in `config/rag.py`
  - Increase `MAX_RETRIEVED_DOCS`
  - Review embedding quality
  - Check if documents are properly indexed

- **Milvus >> GraphRAG:**
  - GraphRAG may need reprocessing
  - Consider adjusting GraphRAG search parameters

- **GraphRAG >> Milvus:**
  - Knowledge graph excels for this query type
  - Consider prioritizing GraphRAG for similar queries

### Groundedness Score

**Range:** 0.0 - 1.0 (Hallucination Detector)

| Score | Meaning | Action |
|-------|---------|--------|
| 0.8 - 1.0 | Fully grounded | âœ… No hallucinations |
| 0.7 - 0.8 | Mostly grounded | Minor unsupported claims |
| 0.5 - 0.7 | Partially grounded | âš ï¸ Review prompt templates |
| 0.0 - 0.5 | Poor groundedness | ğŸš¨ Significant hallucinations |

**Score < 0.7 triggers automatic warning:**
```
âš ï¸ Low groundedness detected: 0.65 (potential hallucination)
```

**Common Issues & Fixes:**

- **Low scores (< 0.7):**
  - Review LLM prompts in `services/prompts/`
  - Ensure prompts instruct to "only use provided context"
  - Increase `MAX_CONTEXT_LENGTH` to provide more evidence
  - Consider switching LLM model
  - Add explicit "say I don't know if uncertain" instruction

- **Frequent hallucinations:**
  - Check if `has_relevant_data` flag is properly handled
  - Review fallback response logic
  - Consider reducing `MAX_RESPONSE_LENGTH`

### Answer Relevance Score

**Range:** 0.0 - 1.0

| Score | Meaning | Action |
|-------|---------|--------|
| 0.8 - 1.0 | Directly addresses question | âœ… Excellent response |
| 0.6 - 0.8 | Mostly on-topic | Good quality |
| 0.4 - 0.6 | Partially addresses question | Review prompt clarity |
| 0.0 - 0.4 | Off-topic or irrelevant | âš ï¸ Investigate query processing |

**Common Issues & Fixes:**

- **Low scores (< 0.6):**
  - Review query preprocessing logic
  - Check if question intent is properly captured
  - Improve prompt templates to focus on user question
  - Verify retrieval is getting right topic

### Overall Score

**Average of all three metrics**

```python
overall_score = (context_relevance + groundedness + answer_relevance) / 3
```

**Quality Flags:**

- `excellent_response`: All three scores â‰¥ 0.8
- `high_quality`: Overall score â‰¥ 0.8
- `potential_hallucination`: Groundedness < 0.7
- `irrelevant_context`: Context relevance < 0.6
- `off_topic_answer`: Answer relevance < 0.6

---

## Dashboard

### Launching

```bash
python scripts/launch_trulens_dashboard.py
```

Or with custom port:
```bash
python scripts/launch_trulens_dashboard.py --port 8080
```

### Features

The TruLens dashboard provides:

1. **Overview Tab**
   - Total evaluations count
   - Average scores for each metric
   - Score distributions (histograms)
   - Trend over time (line charts)

2. **Records Tab**
   - Individual query drill-down
   - View exact inputs and outputs
   - See evaluation reasoning
   - Filter by score ranges

3. **Comparisons Tab**
   - Compare different app versions
   - A/B test results
   - Prompt performance comparison

4. **Feedback Tab**
   - Detailed score breakdowns
   - Statistical analysis
   - Export capabilities

### Dashboard Screenshots

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TruLens Dashboard                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  ğŸ“Š Overview                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Total Evaluations: 156                â”‚     â”‚
â”‚  â”‚                                        â”‚     â”‚
â”‚  â”‚ Avg Context Relevance:    0.82 âœ…     â”‚     â”‚
â”‚  â”‚ Avg Groundedness:         0.88 âœ…     â”‚     â”‚
â”‚  â”‚ Avg Answer Relevance:     0.85 âœ…     â”‚     â”‚
â”‚  â”‚                                        â”‚     â”‚
â”‚  â”‚ Hallucination Rate:       3.2% âš ï¸     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                 â”‚
â”‚  ğŸ“ˆ Trends (Last 7 Days)                        â”‚
â”‚  [Line chart showing score trends]             â”‚
â”‚                                                 â”‚
â”‚  ğŸ“‹ Recent Evaluations                          â”‚
â”‚  [Table with latest queries and scores]        â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Reference

### TruLensService

**File:** `trulens_service.py`

#### Methods

**`async initialize()`**
- Initializes TruLens with feedback providers
- Sets up database connection

**`async evaluate_rag_response(query, retrieved_contexts, generated_answer, ...)`**
- Evaluates a RAG response using the RAG Triad
- **Returns:** `EvaluationScores`

**`get_statistics()`**
- Returns evaluation statistics
- **Returns:** `Dict[str, Any]`

**`launch_dashboard(port=8501)`**
- Launches TruLens web dashboard
- **Blocking call**

#### EvaluationScores Dataclass

```python
@dataclass
class EvaluationScores:
    context_relevance: float              # 0-1
    groundedness: float                   # 0-1
    answer_relevance: float               # 0-1
    overall_score: float                  # 0-1
    milvus_context_relevance: Optional[float]
    graphrag_context_relevance: Optional[float]
    evaluation_time: float                # seconds
    model_used: str                       # e.g., "gpt-4"
```

### RAGEvaluator

**File:** `rag_evaluator.py`

#### Methods

**`async initialize()`**
- Initializes RAG evaluator with TruLens service

**`async evaluate_response(query, chunks, summaries, graph_data, generated_response, ...)`**
- Evaluates RAG response with per-component breakdown
- **Returns:** `Optional[EvaluationScores]`

**`async evaluate_from_rag_response(query, rag_response)`**
- Evaluates from RAGOrchestrator response dict
- **Returns:** `Optional[EvaluationScores]`

**`add_scores_to_response(response, scores)`**
- Adds evaluation data to response dict
- Adds `evaluation` and `quality_flags` fields
- **Returns:** `Dict[str, Any]`

**`get_statistics()`**
- Returns evaluation statistics
- **Returns:** `Dict[str, Any]`

### Statistics Dictionary

```python
{
    "total_evaluations": 156,
    "avg_context_relevance": 0.82,
    "avg_groundedness": 0.88,
    "avg_answer_relevance": 0.85,
    "low_groundedness_count": 5,
    "hallucination_rate": 0.032  # 3.2%
}
```

---

## Configuration

### Environment Variables (Primary Control)

**IMPORTANT:** TruLens is controlled via `.env` file. Default is **disabled**.

Add to `Server/.env`:

```env
# =============================================================================
# TruLens RAG Evaluation
# =============================================================================

# Enable/Disable TruLens (default: false)
TRULENS_ENABLED=true

# Database path
TRULENS_DB_PATH=./data/trulens_evaluations.db

# Evaluation model (gpt-4, gpt-3.5-turbo, or ollama)
TRULENS_EVALUATION_MODEL=gpt-4

# Hallucination threshold (0.0-1.0)
TRULENS_GROUNDEDNESS_THRESHOLD=0.7

# OpenAI API key (optional - uses Ollama/Mixtral if not set)
OPENAI_API_KEY=sk-...
```

**Configuration Notes:**
- **Default:** `TRULENS_ENABLED=false` (no performance impact)
- **To enable:** Set `TRULENS_ENABLED=true`
- **To disable:** Set `TRULENS_ENABLED=false` or omit the variable
- See `.env.example` for all available settings

### Programmatic Control (Optional)

You can also override the `.env` setting in code:

```python
from app.config import get_settings

settings = get_settings()
print(f"TruLens enabled: {settings.TRULENS_ENABLED}")
print(f"DB path: {settings.TRULENS_DB_PATH}")
print(f"Threshold: {settings.TRULENS_GROUNDEDNESS_THRESHOLD}")
```

These settings are automatically loaded from `Server/app/config/features.py`

---

## Performance Considerations

### Evaluation Overhead

- **Per-query evaluation time:** ~500-2000ms
  - Context Relevance: ~150-300ms
  - Groundedness: ~200-800ms
  - Answer Relevance: ~150-300ms

- **Total query time increase:** ~10-20%
  - Your RAG: ~5-10s
  - + TruLens: +1-2s

### Optimization Tips

1. **Run evaluations async** (already implemented)
2. **Sample evaluation** - Don't evaluate every query in high-traffic production:
   ```python
   import random
   if random.random() < 0.1:  # 10% sampling
       scores = await evaluator.evaluate_response(...)
   ```

3. **Use Ollama/Mixtral** for faster (free) evaluation
4. **Batch evaluation** for offline analysis

---

## Troubleshooting

### Issue: TruLens not installed

**Error:**
```
ImportError: No module named 'trulens_eval'
```

**Fix:**
```bash
pip install trulens-eval==0.33.0
```

### Issue: OpenAI API key not found

**Error:**
```
OpenAI API key not configured
```

**Fix:**
Set in `.env`:
```env
OPENAI_API_KEY=sk-your-key-here
```

Or use Ollama provider (already implemented as fallback).

### Issue: Low groundedness scores

**Symptoms:**
- Frequent hallucination warnings
- Groundedness < 0.7

**Debug:**
1. Check LLM prompts - ensure they instruct to use only provided context
2. Review `has_relevant_data` flag handling
3. Inspect actual responses for unsupported claims
4. Increase context length if too restrictive

**Fix:**
```python
# In prompt template
"IMPORTANT: Only use information from the provided context.
If the context doesn't contain relevant information, say so."
```

### Issue: Low context relevance

**Symptoms:**
- Context relevance < 0.6
- Retrieved docs don't match query

**Debug:**
1. Check `SIMILARITY_THRESHOLD` setting
2. Review embedding quality
3. Test retrieval independently

**Fix:**
```python
# In config/rag.py
SIMILARITY_THRESHOLD = 0.05  # Lower threshold (was 0.1)
MAX_RETRIEVED_DOCS = 20      # More documents (was 15)
```

---

## Next Steps

1. âœ… **Install:** `pip install -r requirements.txt`
2. âœ… **Test:** `python tests/test_trulens_integration.py`
3. âœ… **Integrate:** See [INTEGRATION_GUIDE.md](./INTEGRATION_GUIDE.md)
4. âœ… **Monitor:** `python scripts/launch_trulens_dashboard.py`
5. âœ… **Optimize:** Use insights to tune RAG parameters

---

## Additional Resources

- **TruLens Documentation:** https://www.trulens.org/docs/
- **Integration Guide:** [INTEGRATION_GUIDE.md](./INTEGRATION_GUIDE.md)
- **Example Tests:** `tests/test_trulens_integration.py`
- **Dashboard Script:** `scripts/launch_trulens_dashboard.py`

---

**Questions? Issues?**

Check the integration guide or review the test scripts for working examples.
