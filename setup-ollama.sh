#!/bin/bash

echo "ğŸ¤– Setting up Ollama LLM..."
echo ""

# Check if Ollama container is running
if ! docker ps | grep -q neuroclima-ollama; then
    echo "âŒ Ollama container is not running. Please run './start.sh' first."
    exit 1
fi

echo "ğŸ“¥ Downloading Mistral 7B model (this may take a while)..."
docker exec -it neuroclima-ollama ollama pull mistral:7b

echo ""
echo "âœ… Ollama model downloaded successfully!"
echo ""
echo "Available models:"
docker exec neuroclima-ollama ollama list
echo ""
echo "ğŸ’¡ You can now use the application with local LLM support"
